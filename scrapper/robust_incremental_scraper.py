#!/usr/bin/env python3
"""
Robust Incremental Scraper with Cookie Management
Handles cookie expiration, refresh, and recovery automatically
"""

try:
    from incremental_parallel_enhanced_scraper import *
    from cookie_manager import CookieManager
    import argparse
    import sys
    import os
    
    class RobustIncrementalScraper(IncrementalParallelBatchScraper):
        """Enhanced scraper with robust cookie management"""
        
        def __init__(self, cookies_file: str = None, max_workers: int = None):
            super().__init__(cookies_file, max_workers)
            self.cookie_manager = CookieManager(cookies_file or "cookies.json")
            self.retry_count = 0
            self.max_retries = 2
        
        def ensure_valid_cookies(self) -> bool:
            """Ensure we have valid cookies before scraping"""
            print(f"ðŸ” Ensuring valid authentication...")
            
            # Check and refresh cookies if needed
            return self.cookie_manager.refresh_cookies_if_needed()
        
        def handle_authentication_failure(self, error_context: str = "") -> bool:
            """Handle authentication failures during scraping"""
            print(f"\nðŸš¨ AUTHENTICATION FAILURE DETECTED")
            if error_context:
                print(f"Context: {error_context}")
            
            self.retry_count += 1
            
            if self.retry_count <= self.max_retries:
                print(f"ðŸ”„ Attempting cookie refresh (retry {self.retry_count}/{self.max_retries})...")
                
                # Force refresh cookies
                if self.cookie_manager.refresh_cookies_if_needed(force_refresh=True):
                    print(f"âœ… Cookies refreshed - retrying operation")
                    return True
                else:
                    print(f"âŒ Cookie refresh failed")
            else:
                print(f"âŒ Max retries exceeded - giving up")
            
            return False
        
        def scrape_venues_with_recovery(self, venues_file: str, db_file: str, max_pages: int = 10, delay: float = 1.5):
            """Scrape venues with automatic error recovery"""
            
            # Ensure valid cookies before starting
            if not self.ensure_valid_cookies():
                print(f"âŒ Cannot proceed without valid cookies")
                return False
            
            try:
                # Attempt normal scraping
                self.scrape_venues_incremental(venues_file, db_file, max_pages, delay)
                return True
                
            except Exception as e:
                error_msg = str(e).lower()
                
                # Check if error is authentication-related
                auth_errors = [
                    'unauthorized', 'forbidden', 'login required',
                    'authentication failed', 'invalid session',
                    'cookie expired', 'access denied'
                ]
                
                if any(auth_error in error_msg for auth_error in auth_errors):
                    print(f"ðŸ” Detected authentication error: {e}")
                    
                    if self.handle_authentication_failure(str(e)):
                        print(f"ðŸ”„ Retrying scraping with fresh cookies...")
                        try:
                            self.scrape_venues_incremental(venues_file, db_file, max_pages, delay)
                            return True
                        except Exception as retry_e:
                            print(f"âŒ Retry failed: {retry_e}")
                            return False
                    else:
                        print(f"âŒ Could not recover from authentication failure")
                        return False
                else:
                    print(f"âŒ Non-authentication error: {e}")
                    raise e
    
    def setup_cookie_monitoring():
        """Setup cookie monitoring and refresh scheduling"""
        print(f"ðŸ“… Cookie Monitoring Options:")
        print(f"1. Check cookies now and setup monitoring")
        print(f"2. Create cookie refresh script for cron")
        print(f"3. Skip monitoring setup")
        
        choice = input("Choose option (1/2/3): ").strip()
        
        if choice == '1':
            manager = CookieManager()
            manager.refresh_cookies_if_needed()
            
            print(f"\nðŸ“‹ COOKIE MONITORING RECOMMENDATIONS:")
            print(f"Set up a cron job to check cookies daily:")
            print(f"0 6 * * * cd {os.getcwd()} && python3 cookie_manager.py")
            print(f"\nOr run before each scraping session:")
            print(f"python3 cookie_manager.py && python3 robust_incremental_scraper.py")
            
        elif choice == '2':
            create_cookie_refresh_script()
            
    def create_cookie_refresh_script():
        """Create a standalone cookie refresh script"""
        script_content = '''#!/usr/bin/env python3
"""
Automated Cookie Refresh Script
Run this via cron to maintain fresh cookies
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from cookie_manager import CookieManager
from datetime import datetime

def main():
    print(f"ðŸ• Cookie refresh check at {datetime.now()}")
    
    manager = CookieManager()
    
    # Check if cookies need refresh
    cookies = manager.load_cookies()
    if not cookies:
        print("âŒ No cookies found - manual extraction required")
        return False
    
    # Check expiration
    expired, expired_list = manager.check_cookie_expiration(cookies)
    if expired:
        print("â° Cookies expired or expiring soon")
        print("ðŸ“§ Send notification or alert here")
        return False
    
    # Validate cookies
    if manager.validate_cookies_with_request(cookies):
        print("âœ… Cookies are valid")
        return True
    else:
        print("âŒ Cookies invalid - manual refresh required")
        return False

if __name__ == "__main__":
    main()
'''
        
        script_path = "cookie_refresh_cron.py"
        with open(script_path, 'w') as f:
            f.write(script_content)
        
        os.chmod(script_path, 0o755)  # Make executable
        
        print(f"âœ… Created cookie refresh script: {script_path}")
        print(f"\nðŸ“‹ Add this to your crontab:")
        print(f"# Check cookies daily at 6 AM")
        print(f"0 6 * * * cd {os.getcwd()} && python3 {script_path}")
        print(f"\n# Or check cookies before each scraping run")
        print(f"0 */6 * * * cd {os.getcwd()} && python3 {script_path}")
    
    def main():
        parser = argparse.ArgumentParser(description='Robust incremental scraper with cookie management')
        parser.add_argument('--venues', default='venues', help='File containing venue URLs')
        parser.add_argument('--cookies', default='cookies.json', help='Path to cookies file')
        parser.add_argument('--database', default='venues_database.csv', help='Database CSV file to update')
        parser.add_argument('--max-pages', type=int, default=10, help='Max pages per venue')
        parser.add_argument('--delay', type=float, default=1.5, help='Delay between requests')
        parser.add_argument('--workers', type=int, default=None, help='Number of parallel workers')
        parser.add_argument('--full-scrape', action='store_true', help='Force full scrape')
        parser.add_argument('--setup-monitoring', action='store_true', help='Setup cookie monitoring')
        parser.add_argument('--check-cookies', action='store_true', help='Check cookie status only')
        
        args = parser.parse_args()
        
        # Setup monitoring if requested
        if args.setup_monitoring:
            setup_cookie_monitoring()
            return
        
        # Check cookies only if requested
        if args.check_cookies:
            manager = CookieManager(args.cookies)
            valid = manager.refresh_cookies_if_needed()
            if valid:
                print("âœ… Cookies are ready for scraping")
            else:
                print("âŒ Cookie issues detected")
            return
        
        # Validate required files
        if not os.path.exists(args.venues):
            print(f"âŒ Venues file not found: {args.venues}")
            return
        
        if not os.path.exists(args.cookies):
            print(f"âŒ Cookies file not found: {args.cookies}")
            print(f"ðŸ’¡ Run with --setup-monitoring to extract cookies")
            return
        
        # Run robust scraper
        print(f"ðŸš€ ROBUST INCREMENTAL SCRAPER")
        print(f"Cookie file: {args.cookies}")
        print(f"Database: {args.database}")
        print(f"Venues: {args.venues}")
        print("=" * 50)
        
        scraper = RobustIncrementalScraper(cookies_file=args.cookies, max_workers=args.workers)
        
        try:
            success = scraper.scrape_venues_with_recovery(
                venues_file=args.venues,
                db_file=args.database,
                max_pages=args.max_pages,
                delay=args.delay
            )
            
            if success:
                print(f"\nðŸŽ‰ Scraping completed successfully!")
            else:
                print(f"\nâŒ Scraping failed - check authentication")
                sys.exit(1)
                
        except KeyboardInterrupt:
            print(f"\nðŸ›‘ Scraping interrupted by user")
        except Exception as e:
            print(f"\nâŒ Unexpected error: {e}")
            sys.exit(1)
    
    if __name__ == "__main__":
        main()

except ImportError as e:
    print(f"âŒ Missing dependency: {e}")
    print("Install with: pip install selenium beautifulsoup4 requests pandas python-dateutil")
    sys.exit(1)